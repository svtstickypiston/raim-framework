{
    "id": 42,
    "feature_num": "01",
    "feature_name": "Audit Access",
    "pillar_id": 7,
    "pillar": "Accountability",
    "facet": "Auditability",
    "feature_brief": "If the system is proprietary, independent auditors can access the model and training data to evaluate it.",
    "feature_summary": "It may be important for a proprietary generative system to ensure auditability, and allow internal and external auditors access upon request to assess its compliance with the previously defined responsible features. These audit reports could be publicly available.",
    "feature_use_case": "This is about allowing for independent checks and balances.  Even if the AI system is not open-source, trusted third parties should be able to examine it to ensure that it's meeting ethical and legal standards.  Their findings should be made public, providing an extra layer of accountability.",
    "feature_long": "Even if a system is not fully open-source, accountability requires allowing independent scrutiny. This feature means that, upon request, authorised auditors (either within the developing organisation or from external bodies) can access the model's code, training data, and other relevant information. This allows them to assess the system for potential biases, ethical concerns, or compliance with regulations. Making the audit reports available (while respecting confidentiality where necessary) further enhances transparency and builds trust."
}